[Data]
src = en
tgt = fr
train_prefix =  bpe/train
dev_prefix = bpe/dev
test_prefix = bpe/test
vocab_prefix = bpe/vocab.bpe.32000
out_dir = /tmp/saved

[Networks]
num_units = 128
num_layers = 2
dropout = 0.2
unit_type = lstm
use_peepholes = True
encoder_type = bi
num_residual_layers = 0
time_major = True
num_embeddings_partitions = 0

[Attention]
attention = normed_bahdanau
attention_architecture = standard
pass_hidden_state = True

[Train]
optimizer = adam
num_train_epochs = 8
num_train_steps = 340000
batch_size = 32
init_op = uniform
init_weight =  0.05
max_gradient_norm = 5.0
learning_rate = 0.05
start_decay_step = 170000
decay_factor = 0.5
decay_steps = 17000
colocate_gradients_with_ops = True

[Data_constraints]
num_buckets = 5
max_train = 0
src_max_len = 80
tgt_max_len = 80
source_reverse = False

[Inference]
src_max_len_infer = 1000
tgt_max_len_infer = 1000
infer_batch_size = 32
beam_width = 10
length_penalty_weight = 1.0

[Vocab]
sos = <s>
eos = </s>
bpe_delimiter = @@
src_vocab_size = 35465
tgt_vocab_size = 35465
src_embed_size = 1024
tgt_embed_size = 1024

[Misc]
forget_bias = 1.0
gpu_list = [ 0 ]
epoch_step = 0
steps_per_stats = 100
steps_per_external_eval = 0
share_vocab = True
metrics = [ 'bleu' ]
log_device_placement = True
random_seed = 3
maximum_iterations = 200
save_path = saved/model1

[Serving]
export_path = exported/1
model_name = nmt
host = 10.12.7.107
port = 9000
requested_threshold = 0.5
request_timeout = 3.0

[Others]
normal_src_vocab_size = 995077
normal_tgt_vocab_size = 1031959
bpe_src_vocab_size = 35465
bpe_tgt_vocab_size = 35465
